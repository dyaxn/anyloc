#!/usr/bin/env python3
"""
ç”Ÿæˆè‡ªå®šä¹‰VLADè¯æ±‡è¡¨ï¼ˆä½¿ç”¨322Ã—322åˆ†è¾¨ç‡ï¼‰
åŸºäºAnyLocçš„å†…å­˜ä¼˜åŒ–ç­–ç•¥
"""

import sys
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from sklearn.cluster import MiniBatchKMeans
import yaml
import random
from PIL import Image
import torchvision.transforms as T
import torch.nn.functional as F
import gc
import time

sys.path.append(str(Path(__file__).parent.parent))

from src.feature.extractor import DinoV2Extractor


class VocabularyGenerator:
    """è¯æ±‡è¡¨ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 model_name: str = "dinov2_vitl14",
                 layer: int = 23,
                 facet: str = "value",
                 device: str = "cuda",
                 resize_dim: int = 322):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_name: DINOv2æ¨¡å‹åç§°
            layer: æå–å±‚
            facet: ç‰¹å¾åˆ†æ”¯ 
            device: è®¡ç®—è®¾å¤‡
            resize_dim: å›¾åƒresizeå°ºå¯¸ï¼ˆ322æ˜¯14çš„å€æ•°ï¼‰
        """
        self.device = device
        
        # ç¡®ä¿æ˜¯14çš„å€æ•°
        self.resize_dim = (resize_dim // 14) * 14
        if self.resize_dim != resize_dim:
            print(f"âš ï¸ Adjusted resize from {resize_dim} to {self.resize_dim} (multiple of 14)")
        
        self.patch_size = 14
        
        # è®¡ç®—patchesæ•°é‡
        self.n_patches = (self.resize_dim // self.patch_size) ** 2
        print(f"Image size: {self.resize_dim}Ã—{self.resize_dim}")
        print(f"Patches per image: {self.n_patches}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("Loading DINOv2 model...")
        self.extractor = DinoV2Extractor(
            model_name=model_name,
            layer=layer,
            device=device
        )
        
        # å›¾åƒé¢„å¤„ç†
        self.transform = T.Compose([
            T.Resize((self.resize_dim, self.resize_dim)),
            T.ToTensor(),
            T.Normalize(mean=[0.485, 0.456, 0.406],
                       std=[0.229, 0.224, 0.225])
        ])
    
    def extract_features_batch(self, image_paths: list, batch_size: int = 16):
        """
        æ‰¹é‡æå–å›¾åƒç‰¹å¾ï¼ˆå¸¦è¯¦ç»†è¿›åº¦ï¼‰
        
        Args:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æ‰€æœ‰å›¾åƒçš„å±€éƒ¨ç‰¹å¾ [N_total_patches, D]
        """
        print("\nInitializing model...")
        self.extractor.init_model()
        print("Model ready!")
        
        all_features = []
        total_images = len(image_paths)
        total_batches = (total_images + batch_size - 1) // batch_size
        
        # ä¸»è¿›åº¦æ¡
        with tqdm(total=total_images, desc="Extracting features", unit="img") as pbar:
            start_time = time.time()
            
            for batch_idx in range(0, total_images, batch_size):
                batch_end = min(batch_idx + batch_size, total_images)
                batch_paths = image_paths[batch_idx:batch_end]
                current_batch_size = len(batch_paths)
                
                # åŠ è½½å›¾åƒ
                batch_tensors = []
                for img_path in batch_paths:
                    img = Image.open(img_path)
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    img_tensor = self.transform(img)
                    batch_tensors.append(img_tensor)
                
                # æ‰¹é‡æ¨ç†
                batch = torch.stack(batch_tensors).to(self.device)
                
                with torch.no_grad():
                    # æå–ç‰¹å¾ [B, N_patches, D]
                    features = self.extractor.extract(batch)
                    # å±•å¹³æ‰¹æ¬¡ç»´åº¦ [B*N_patches, D]
                    features = features.reshape(-1, features.shape[-1])
                    all_features.append(features.cpu())
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(current_batch_size)
                
                # æ˜¾ç¤ºé¢å¤–ä¿¡æ¯
                elapsed = time.time() - start_time
                avg_speed = (batch_idx + current_batch_size) / elapsed
                eta = (total_images - batch_idx - current_batch_size) / avg_speed if avg_speed > 0 else 0
                
                pbar.set_postfix({
                    'batch': f"{batch_idx//batch_size + 1}/{total_batches}",
                    'speed': f"{avg_speed:.1f} img/s",
                    'ETA': f"{eta:.0f}s"
                })
                
                # æ¸…ç†GPUå†…å­˜
                if self.device == 'cuda' and (batch_idx // batch_size) % 10 == 0:
                    torch.cuda.empty_cache()
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        print("\nConcatenating features...")
        all_features = torch.cat(all_features, dim=0)
        
        return all_features
    
    def cluster_features(self, features: torch.Tensor, n_clusters: int = 32):
        """
        K-meansèšç±»ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…inertia_é”™è¯¯ï¼‰
        
        Args:
            features: ç‰¹å¾å¼ é‡ [N_total_patches, D]
            n_clusters: èšç±»æ•°é‡
            
        Returns:
            èšç±»ä¸­å¿ƒ [K, D]
        """
        print(f"\n{'='*40}")
        print(f"Clustering {features.shape[0]:,} features into {n_clusters} clusters")
        print(f"{'='*40}")
        
        features_np = features.numpy().astype('float32')
        
        # è®¡ç®—æ‰¹æ¬¡å¤§å°
        batch_size = min(10000, features.shape[0])
        print(f"Batch size: {batch_size}")
        print(f"Feature shape: {features_np.shape}")
        print("\nStarting K-means clustering...")
        print("This may take a few minutes...\n")
        
        # åˆ›å»ºMiniBatchKMeans
        kmeans = MiniBatchKMeans(
            n_clusters=n_clusters,
            batch_size=batch_size,
            max_iter=100,
            n_init=3,
            random_state=42,
            verbose=1,  # å¼€å¯å†…ç½®è¿›åº¦æ˜¾ç¤º
            compute_labels=False  # ä¸è®¡ç®—æ ‡ç­¾ï¼ŒèŠ‚çœå†…å­˜
        )
        
        # ç›´æ¥fitï¼ˆä¼šæ˜¾ç¤ºå†…ç½®è¿›åº¦ï¼‰
        kmeans.fit(features_np)
        
        # è·å–èšç±»ä¸­å¿ƒ
        centers = torch.from_numpy(kmeans.cluster_centers_)
        
        print(f"\nâœ“ Clustering complete!")
        print(f"  Centers shape: {centers.shape}")
        print(f"  Final inertia: {kmeans.inertia_:.2f}")
        
        return centers


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Generate VLAD vocabulary")
    parser.add_argument('--data_dir', type=str, 
                       default='/root/autodl-tmp/data/source_tiles/img',
                       help='Database images directory')
    parser.add_argument('--n_clusters', type=int, default=32,
                       help='Number of VLAD clusters')
    parser.add_argument('--max_images', type=int, default=6800,
                       help='Maximum images to use')
    parser.add_argument('--subsample', type=int, default=1,
                       help='Subsample factor (1=use all, 2=use half, etc.)')
    parser.add_argument('--batch_size', type=int, default=16,
                       help='Batch size for feature extraction')
    parser.add_argument('--output', type=str, 
                       default='models/custom_322/c_centers.pt',
                       help='Output vocabulary file')
    parser.add_argument('--resize', type=int, default=322,
                       help='Resize dimension (322 for clustering, 896 for retrieval)')
    args = parser.parse_args()
    
    print("="*60)
    print("VLAD Vocabulary Generation (Memory Optimized)")
    print("="*60)
    
    # è·å–å›¾åƒåˆ—è¡¨
    print("\nğŸ“ Scanning image directory...")
    image_dir = Path(args.data_dir)
    image_extensions = ['*.jpg', '*.jpeg', '*.png']
    
    image_paths = []
    for ext in image_extensions:
        found = list(image_dir.glob(ext))
        image_paths.extend(found)
    
    image_paths = sorted(image_paths)[:args.max_images]
    print(f"âœ“ Found {len(image_paths)} images")
    
    # é™é‡‡æ ·
    if args.subsample > 1:
        print(f"\nğŸ“‰ Subsampling with factor {args.subsample}")
        image_paths = image_paths[::args.subsample]
        print(f"âœ“ After subsampling: {len(image_paths)} images")
    
    # é™åˆ¶æœ€å¤§å›¾åƒæ•°
    if len(image_paths) > args.max_images:
        print(f"\nğŸ² Limiting to {args.max_images} images")
        image_paths = image_paths[:args.max_images]
    
    print(f"\nğŸ“Š Final dataset: {len(image_paths)} images")
    
    # å†…å­˜ä¼°ç®—
    print("\nğŸ’¾ Memory estimation:")
    resize_actual = (args.resize // 14) * 14
    patches_per_img = (resize_actual // 14) ** 2
    mem_per_img = patches_per_img * 1536 * 4 / (1024**2)  # MB
    total_mem = mem_per_img * len(image_paths) / 1024  # GB
    print(f"  Resize dimension: {resize_actual}Ã—{resize_actual}")
    print(f"  Patches per image: {patches_per_img}")
    print(f"  Memory per image: {mem_per_img:.2f} MB")
    print(f"  Total estimated: {total_mem:.2f} GB")
    
    if total_mem > 50:
        print(f"âš ï¸  Warning: Estimated memory usage is high ({total_mem:.1f}GB)")
        print("   Consider using --subsample or reducing --max_images")
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    print("\nğŸš€ Initializing generator...")
    generator = VocabularyGenerator(
        model_name="dinov2_vitl14",
        layer=23,
        facet="value",
        device="cuda" if torch.cuda.is_available() else "cpu",
        resize_dim=args.resize
    )
    print("âœ“ Generator ready")
    
    # æå–ç‰¹å¾
    print("\n" + "="*40)
    print("Step 1/3: Feature Extraction")
    print("="*40)
    
    start_time = time.time()
    features = generator.extract_features_batch(
        image_paths, 
        batch_size=args.batch_size
    )
    
    extraction_time = time.time() - start_time
    print(f"\nâœ“ Feature extraction complete!")
    print(f"  Shape: {features.shape}")
    print(f"  Memory: {features.nbytes / (1024**3):.2f} GB")
    print(f"  Time: {extraction_time:.1f} seconds")
    print(f"  Speed: {len(image_paths)/extraction_time:.1f} img/s")
    
    # K-meansèšç±»
    print("\n" + "="*40)
    print("Step 2/3: K-means Clustering")
    print("="*40)
    
    start_time = time.time()
    centers = generator.cluster_features(features, n_clusters=args.n_clusters)
    clustering_time = time.time() - start_time
    
    print(f"  Time: {clustering_time:.1f} seconds")
    
    # æ¸…ç†å†…å­˜
    print("\nğŸ§¹ Cleaning memory...")
    del features
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    print("âœ“ Memory cleaned")
    
    # ä¿å­˜è¯æ±‡è¡¨
    print("\n" + "="*40)
    print("Step 3/3: Saving Vocabulary")
    print("="*40)
    
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜æ ¼å¼ä¸AnyLocä¸€è‡´
    vocab_data = {
        'c_centers': centers,
        'n_clusters': args.n_clusters,
        'feature_dim': centers.shape[1],
        'n_training_images': len(image_paths),
        'resize_dim': generator.resize_dim,  # ä½¿ç”¨å®é™…çš„resizeç»´åº¦
        'source_dir': str(args.data_dir),
        'config': {
            'model': 'dinov2_vitl14',
            'layer': 23,
            'facet': 'value'
        }
    }
    
    print("ğŸ’¾ Saving vocabulary...")
    torch.save(vocab_data, output_path)
    print(f"âœ… Vocabulary saved to: {output_path}")
    
    # æ˜¾ç¤ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“‹ Summary")
    print("="*60)
    print(f"  Clusters: {args.n_clusters}")
    print(f"  Feature dimension: {centers.shape[1]}")
    print(f"  Training images: {len(image_paths)}")
    print(f"  Total time: {extraction_time + clustering_time:.1f} seconds")
    
    # éªŒè¯åŠ è½½
    print("\nğŸ” Verifying saved file...")
    loaded = torch.load(output_path)
    print(f"âœ“ Verification successful - centers shape: {loaded['c_centers'].shape}")


if __name__ == "__main__":
    main()